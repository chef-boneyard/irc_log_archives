[2012-08-11 03:24:11 -0400] ssd7: Ohai Chefs!
[2012-08-11 05:16:04 -0400] aravind: hi
[2012-08-11 11:15:36 -0400] tangleofwire: I've been following along with the video at http://blip.tv/opscode/getting-started-with-chef-on-the-opscode-platform-3879817 but I end up with an authentication when I run chef-client for the first time
[2012-08-11 11:16:24 -0400] tangleofwire: It looks like I'm getting a 403 from the opscode servers, but my validation.pem in /etc/chef is the same as my org validation key: https://gist.github.com/3323859
[2012-08-11 11:16:47 -0400] tangleofwire: Can anyone point me in the direction of a fix?
[2012-08-11 13:59:10 -0400] _aeris_: hello #chef
[2012-08-11 13:59:14 -0400] _aeris_: i have a question
[2012-08-11 13:59:31 -0400] _aeris_: is there possible for a node to access another node attributes ?
[2012-08-11 13:59:54 -0400] flaccid_: i think via search
[2012-08-11 13:59:58 -0400] _rc: to read them, sure; just search for the other node
[2012-08-11 14:00:10 -0400] _aeris_: in my case, i use linux container, one of them is a dns server and all others containers must look for it ip for resolv
[2012-08-11 14:01:06 -0400] _aeris_: yeah !
[2012-08-11 14:01:16 -0400] _aeris_: seems this is what i search
[2012-08-11 14:01:18 -0400] _aeris_: thanks
[2012-08-11 14:44:02 -0400] rayrod2030: do you guys tend to check in your .chef directory into source?  For example things like cookbook checksums?
[2012-08-11 15:29:16 -0400] cheeseplus: rayrod2030: I've not ever checked in the checksums
[2012-08-11 15:29:31 -0400] rayrod2030: do you just ignore the entire .chef directory all together?
[2012-08-11 15:29:54 -0400] cheeseplus: there might be some value in doing so if you are expecting something nefarious
[2012-08-11 15:30:12 -0400] rayrod2030: good deal.  thanks.
[2012-08-11 15:31:54 -0400] cheeseplus: yea, it seems like in all our repos we don't check it in
[2012-08-11 15:32:32 -0400] cheeseplus: but if you wanted to rule out crazy things messing with hashes, it might make sense (or at least wouldn't hurt to revision them)
[2012-08-11 16:13:42 -0400] meisen: hello #chef ... is there an easy way to "guard" an entire recipe (i.e. exit out of the recipe if a node property is already set) without guarding each resource with not_if or only_if?
[2012-08-11 16:56:42 -0400] jtimberman: meisen: sure, put a conditional at the top and then 'return' if you want to continue with the run, or 'raise' if you want to exit the chef run completel
[2012-08-11 17:17:11 -0400] fractaloop: So I've read the logs, and I'm aware that jtimberman hates the json gem and that everyone wants it gone. Is there a timeline for this?
[2012-08-11 17:26:26 -0400] jtimberman: fractaloop: lol
[2012-08-11 17:26:44 -0400] jtimberman: its more that the backwards-compatibility breaking changes that gem seems to push frequently frustrates me to no end.
[2012-08-11 17:27:14 -0400] fractaloop: Well it is owned by a Nutter ;)
[2012-08-11 17:27:17 -0400] jtimberman: or at least, pushed frequently historically, causing everyone who uses it to put paranoid version contraints on it.
[2012-08-11 17:28:28 -0400] fractaloop: It's just that Rails has been pegged at 1.7.4 since 3.2 (now on 3.2.8) and I can't upgrade my Rails projects anymore =(
[2012-08-11 17:28:37 -0400] fractaloop: Is the transition to Yajl going well?
[2012-08-11 17:28:57 -0400] jtimberman: i don't know, i haven't been involved in that.
[2012-08-11 17:29:11 -0400] jtimberman: that kind of thing is exactly what i'm talking about.
[2012-08-11 17:29:45 -0400] jtimberman: only ticket i'm really seeing is here: http://tickets.opscode.com/browse/CHEF-1948
[2012-08-11 17:29:53 -0400] jtimberman: but i'm not searching in earnest:)
[2012-08-11 17:30:00 -0400] jtimberman: perhaps a post to chef-dev mailing list is in order?
[2012-08-11 17:31:20 -0400] fractaloop: I'll give it a shot.
[2012-08-11 17:33:33 -0400] fractaloop: Thanks
[2012-08-11 18:12:23 -0400] hakunin: Having carefully separated my users into runner, deployer, and provisioner (root/ubuntu in my case) (none of whom are admins), is it normal that i need to use the provisioner to restart services running via bluepill?
[2012-08-11 18:13:11 -0400] hakunin: i'm not a sysadmin, trying to figure out good practices
[2012-08-11 18:28:23 -0400] cheeseplus: hakunin: not familiar with bluepill but it's common for certain init scripts to want to be root before "su"ing to a user
[2012-08-11 19:03:51 -0400] hakunin: cheeseplus: i see. with capistrano setup it means all these ssh reconnects :(
[2012-08-11 19:06:31 -0400] hoover_damm: hakunin, bluepill needs root unless it's setup in non-priviledged mode
[2012-08-11 19:06:43 -0400] hoover_damm: hakunin, unfortunately the process management problem is not fun
[2012-08-11 19:06:58 -0400] hoover_damm: hakunin, god expects root so does monit (or it expects to be the same owner as monitrc)
[2012-08-11 19:07:26 -0400] hoover_damm: hakunin, so you end up requiring passwordless sudo (or passworded)
[2012-08-11 19:07:34 -0400] hakunin: hoover_damm: i see. but in case of monit, i don't think you actually use it to control daemons. Bluepill encourages using itself for control
[2012-08-11 19:07:39 -0400] hoover_damm: hakunin, I recently tried to shoot down our passwordless sudo and remove it
[2012-08-11 19:07:44 -0400] hoover_damm: ended up adding it back
[2012-08-11 19:07:48 -0400] hoover_damm: and ensuring it was there
[2012-08-11 19:07:56 -0400] hoover_damm: we were just not ready
[2012-08-11 19:08:21 -0400] hoover_damm: hakunin, actually monit gets just as pissy
[2012-08-11 19:08:59 -0400] hakunin: hoover_damm: i mean, with monit you just launch and forget. With bluepill, you also use it to start/stop/restart processes, and every time you have to sudo for those
[2012-08-11 19:09:26 -0400] hoover_damm: hakunin, as I was the person who more or less made Unicorn work with Monit (at Engine Yard) I can tell you personally that there were many times when we tried to control it without monit to only have monit start the process during a restart.
[2012-08-11 19:09:40 -0400] hoover_damm: (more then less)
[2012-08-11 19:09:58 -0400] hoover_damm: hakunin, i've implemented unicorn in Bluepill and in God and in Monit.  God and Bluepill were the least ugly offenders
[2012-08-11 19:10:11 -0400] hakunin: hoover_damm: i see
[2012-08-11 19:10:15 -0400] hoover_damm: hakunin, I'm going to evaluate s6 and systemd sometime as an alternative
[2012-08-11 19:10:36 -0400] hoover_damm: hakunin, because basically the methods I listed above of process management work... but not great
[2012-08-11 19:11:00 -0400] hoover_damm: hakunin, and there either has to be something better or it's time to see how badly I can fuck that up :)
[2012-08-11 19:11:16 -0400] hakunin: : )
[2012-08-11 19:11:37 -0400] hakunin: so i can calm down about having to sudo, it's not unusual
[2012-08-11 19:12:33 -0400] hakunin: hoover_damm: I'm about to post another question on serverfault, but might as well throw it at you. Wondering whether I should have multipel nginx+unicorn instances behind a HAProxy/ELB/nginx load balancer or multiple pure-unicorn instances behind nginx as load-balancer. not sure why go either...
[2012-08-11 19:12:49 -0400] hoover_damm: hakunin, it's shitty, but normal
[2012-08-11 19:13:13 -0400] hoover_damm: hakunin, honestly i usually have 1 or two frontends for static content
[2012-08-11 19:13:16 -0400] cheeseplus: hakunin: yea, in a perfect world we'd not have this problem
[2012-08-11 19:13:17 -0400] hoover_damm: hakunin, and the rest are workers
[2012-08-11 19:13:30 -0400] hoover_damm: hakunin, in a perfect world i'd have elb -> nginx -> haproxy -> unicorns
[2012-08-11 19:14:07 -0400] hakunin: hoover_damm: i did research haproxy vs nginx as load balancer, and i'm not caring about performance difference enough to introduce haproxy where it doesn't have to be
[2012-08-11 19:14:19 -0400] hakunin: hoover_damm: but then why have elb -> nginx part?
[2012-08-11 19:14:32 -0400] hakunin: wouldn't there just be 1 nginx?
[2012-08-11 19:14:33 -0400] hoover_damm: hakunin, sorry server incase your server goes down
[2012-08-11 19:14:54 -0400] hakunin: hoover_damm: ah
[2012-08-11 19:15:06 -0400] stormerider: for static content why not just push it to cf if you're using AWS?
[2012-08-11 19:15:06 -0400] cheeseplus: in my experience haproxy is much better when acting as a backend, in rack load balancer
[2012-08-11 19:15:18 -0400] digitallogic: does anyone know how often http://s3.amazonaws.com/chef-solo/bootstrap-latest.tar.gz gets updated?
[2012-08-11 19:15:19 -0400] cheeseplus: at least that is where it shines over nginx
[2012-08-11 19:15:34 -0400] hakunin: stormerider: will do, that's probably gonna be next step, i'm tasked to setup this whole thing kind of over the weekend so picking my battles : )
[2012-08-11 19:15:42 -0400] digitallogic: I'm hitting a bug in the java cookbook that appears to have been fixed on githum
[2012-08-11 19:15:46 -0400] digitallogic: * github
[2012-08-11 19:15:58 -0400] stormerider: hakunin: oh, i so hear that (just coming out of crunch time myself)
[2012-08-11 19:16:52 -0400] hoover_damm: anywho your really not doing load balancing
[2012-08-11 19:16:55 -0400] hoover_damm: until you do enterprise
[2012-08-11 19:16:57 -0400] hoover_damm: haproxy isn't lb
[2012-08-11 19:17:02 -0400] hoover_damm: it's more packet passing
[2012-08-11 19:17:39 -0400] hakunin: so if i'm not going for failoverand i don't care much about the nginx/haproxy difference as a load balancer (the stats show haproxy is faster, but not enough to make it worth my time at this point), then nginx -> unicorns would be simplest way that works well enough
[2012-08-11 19:17:48 -0400] hoover_damm: load balancers generally do a lot more then just passing http packets between a few hosts
[2012-08-11 19:17:57 -0400] hoover_damm: i'm against nginx mainly because of it's event / threading mixture
[2012-08-11 19:18:21 -0400] stormerider: i like nginx because it actually supports ssl backends *sighs at haproxy*
[2012-08-11 19:18:35 -0400] hoover_damm: problem with nginx is it's evented and threading
[2012-08-11 19:18:42 -0400] hoover_damm: so you get blackmarks in your cpu time
[2012-08-11 19:18:43 -0400] cheeseplus: hoover_damm: we use harproxy all over the place for riak lb
[2012-08-11 19:18:50 -0400] cheeseplus: it's the only thing that keeps up on 10Gbit
[2012-08-11 19:18:53 -0400] hoover_damm: cheeseplus, how's your reconnect?
[2012-08-11 19:19:02 -0400] hoover_damm: reconnecting every day?
[2012-08-11 19:19:07 -0400] cheeseplus: stormerider: for that we use stud + haproxy
[2012-08-11 19:19:24 -0400] hoover_damm: cheeseplus, LOL I basically benchmarked and theorized haproxy when Riak was around 0.14 and got it working
[2012-08-11 19:19:29 -0400] stormerider: havent heard of stud before, im guessing it's like stunnel?
[2012-08-11 19:19:33 -0400] hoover_damm: cheeseplus, and found out every day on my connection timeout my connections has to recnnect
[2012-08-11 19:19:40 -0400] hoover_damm: cheeseplus, not a horrible pain
[2012-08-11 19:19:42 -0400] cheeseplus: hoover_damm: hrm, not run into that
[2012-08-11 19:19:43 -0400] hoover_damm: cheeseplus, but just :o
[2012-08-11 19:19:47 -0400] hoover_damm: cheeseplus, you have
[2012-08-11 19:19:50 -0400] hoover_damm: cheeseplus, drivers reconnect :)
[2012-08-11 19:20:04 -0400] hakunin: hoover_damm: stormerider: none of that seems like something that would impact my app at this stage, so nginx -> unicorns should be enough then...
[2012-08-11 19:20:08 -0400] hoover_damm: cheeseplus, basically with http you are more graceful but with protobuffers absofreakinglutely
[2012-08-11 19:20:13 -0400] hoover_damm: cheeseplus, but you might not be lb'ing your pbc
[2012-08-11 19:20:50 -0400] hoover_damm: and thus hakunin creates his first technical debt
[2012-08-11 19:20:59 -0400] hakunin: hoover_damm: not mine
[2012-08-11 19:21:01 -0400] hoover_damm: hakunin, be proud of your technical debt baby :)
[2012-08-11 19:21:13 -0400] hakunin: hoover_damm: a person we will hire to do our sysadmining
[2012-08-11 19:21:16 -0400] hoover_damm: hakunin, technical debt is created every day of our lives
[2012-08-11 19:21:34 -0400] stormerider: remind me what technical debt is again? been a while since ive heard the term
[2012-08-11 19:21:34 -0400] hoover_damm: hakunin, we can either try and stem the flow as much as we can... or let it run over us like a raging river
[2012-08-11 19:21:40 -0400] hakunin: hoover_damm: but then how much debt is there, when the person could just build out the whole thing from scratch in our AWS account, and switch the whole site painlessly
[2012-08-11 19:22:14 -0400] hoover_damm: hakunin, no one ever tell you the sysadmin joke about 3 envelopes?
[2012-08-11 19:22:23 -0400] cheeseplus: hoover_damm: I'd have to check our configs, we might very well be only using it for http
[2012-08-11 19:22:33 -0400] hakunin: hoover_damm: in order for technical debt to exist you need technical revenue
[2012-08-11 19:22:39 -0400] cheeseplus: hehe
[2012-08-11 19:23:26 -0400] hakunin: hoover_damm: but jokes aside what would you recommend me do to avoid technical debt? go nginx -> haproxy -> unicorns rather than nginx -> unicorns?
[2012-08-11 19:23:35 -0400] hakunin: wouldn't it be a trivial change to insert it in there for a sysadmin?
[2012-08-11 19:24:30 -0400] hoover_damm: hakunin, the joke went you got hired as a new sysadmin... old one was leaving and he told you he left you 3 envelopes in case of emergency... 6 months later the networking starts going to heck and you can't figure out what's wrong... you remember back to your first day and open the first evelope and it says 'Blame everything on me'
[2012-08-11 19:24:57 -0400] hoover_damm: hakunin, so you got o your bosses and blame it on the old guy and his configs were just janked up
[2012-08-11 19:25:22 -0400] hakunin: hoover_damm: go on, i'm listening :)
[2012-08-11 19:25:28 -0400] hoover_damm: hakunin, they said okay, you fixed it... a year later... hell opens and your servers are on fire... and you open the next evelope and it says 'Blame it all on old hardware'
[2012-08-11 19:25:49 -0400] hoover_damm: hakunin, you go to your bosses and blame it on the hardware... they give you the budget to buy new gear... your happy for 6months
[2012-08-11 19:26:20 -0400] hoover_damm: hakunin, then you open the last evelope and it reads 'Please open the desk drawer and pick out 3 envelopes and place the same 3 messages in them' pack your desk and get ready for your replacement.
[2012-08-11 19:26:39 -0400] hakunin: heh
[2012-08-11 19:26:58 -0400] hoover_damm: hakunin, really the motto is we are all dispensable and we should create as little technical debt as possible (if we want)
[2012-08-11 19:27:26 -0400] hoover_damm: hakunin, and to remember your replaceable just like he was :)
[2012-08-11 19:27:28 -0400] stormerider: i mean, if i find that nginx doesnt keep up with what i want it to do, then i can just set up another cluster with haproxy+stunnel or something along those lines and repoint the ELB and walk away. at that point i think it's more performance tuning than debt per se, since refactoring time is localized and mild IMO.
[2012-08-11 19:27:43 -0400] hakunin: i'm a founder of this startup, and only doing admin stuff because we can't find a good one, and i truly care to exert as much effort as possible to learn this stuff
[2012-08-11 19:28:10 -0400] hoover_damm: hakunin, well if your the founder then basically you get care the most :(
[2012-08-11 19:28:34 -0400] hoover_damm: hakunin, as you likely will be there for awhile at least ensuring it gets where you want to and more
[2012-08-11 19:28:46 -0400] hakunin: so..
[2012-08-11 19:28:56 -0400] hakunin: as i mentioned
[2012-08-11 19:29:00 -0400] hakunin: hoover_damm: "what would you recommend me do to avoid technical debt? go nginx -> haproxy -> unicorns rather than nginx -> unicorns?"
[2012-08-11 19:29:07 -0400] hoover_damm: if you want to do SSL Termination you should do Apache... Nginx is acceptable for regular http, haproxy is good
[2012-08-11 19:29:25 -0400] hoover_damm: keep the amount of hoops down
[2012-08-11 19:29:28 -0400] hoover_damm: and disable syncookies
[2012-08-11 19:29:42 -0400] hoover_damm: that's the first technical debt that gets created that almost no one knows about
[2012-08-11 19:29:43 -0400] stormerider: hakunin: are you using nginx as a web server or as a proxy?
[2012-08-11 19:30:07 -0400] hakunin: stormerider: nginx as a web server with unicorn instances directly behind it
[2012-08-11 19:30:14 -0400] stormerider: ah ok.
[2012-08-11 19:30:23 -0400] hakunin: stormerider: that's the proposal i mean
[2012-08-11 19:30:36 -0400] hoover_damm: net.ipv4.tcp_syncookies
[2012-08-11 19:31:02 -0400] hoover_damm: my client has recently disabled syncookies as they realized it was hurting them (the kernel throttle/choke)
[2012-08-11 19:31:10 -0400] hoover_damm: and saved a ton of latency :)
[2012-08-11 19:31:21 -0400] stormerider: so elb -> nginx -> unicorns makes more sense to me than elb -> nginx -> haproxy -> unicorns, but i dont know what unicorns is so i could be wrong. it just seems like its adding an extra layer you dont need since the elb is handling load balancing.
[2012-08-11 19:31:29 -0400] hoover_damm: that has to be one of the worst features of linux ever... get too many packets and let's start inspecting and increase the load
[2012-08-11 19:31:32 -0400] hoover_damm: YEAH!
[2012-08-11 19:31:34 -0400] hoover_damm: ...
[2012-08-11 19:31:55 -0400] cheeseplus: hoover_damm: +a billion for disabling synccookoes
[2012-08-11 19:32:07 -0400] cheeseplus: or however spelling works
[2012-08-11 19:33:57 -0400] hakunin: let me describe my situation. My app currently runs on 2 large instances, one with nginx+passenger+postgres, the other with resque workers, etc. That's it. It's doing ok. The setup is not optimized for scaling and a bit messy, so i'm just trying to do a simple thing that could work with my limited knowledge. This is not a matter of optimizing hundreds of requests per second, this is a matter of getting an app running relatively
[2012-08-11 19:33:57 -0400] hakunin: cleanly and properly with minimal thing that can work.
[2012-08-11 19:34:52 -0400] hakunin: frankly i don't know what ssl termination, hoops, and syncookies mean, can't tell if it will have any impact on our app at this stage
[2012-08-11 19:34:55 -0400] hakunin: but we're pressed for time
[2012-08-11 19:35:23 -0400] hakunin: and i'm trying my best to find my way in this deep forest
[2012-08-11 19:35:51 -0400] stormerider: irt SSL... if you're on AWS and using ELBs, have the ELB do SSL and ignore it yourself. simple and efficient.
[2012-08-11 19:36:06 -0400] hakunin: i'm unable to answer questions like: which instances should i use? how to split services among them? how many unicorn workers should i run per instance? how am i gonna setup monitoring? what to monitor? which tools to use?
[2012-08-11 19:36:50 -0400] hakunin: :(
[2012-08-11 19:37:29 -0400] stormerider: you should be able to do some basic monitoring with cloudwatch fairly easily, that should bide you enough time until you find a sysadmin who can implement something better.
[2012-08-11 19:37:44 -0400] cheeseplus: hakunin: It comes down to your priorities at the time
[2012-08-11 19:38:04 -0400] cheeseplus: obviously it's not going to be ideal so solve the most immediate problem now, iterate when you have bandwidth or time
[2012-08-11 19:38:14 -0400] cheeseplus: or staff
[2012-08-11 19:38:21 -0400] stormerider: sorry, im only answering the questions that i can see obvious answers to, because in this case i think no information is better than wrong
[2012-08-11 19:38:26 -0400] stormerider: cheeseplus++
[2012-08-11 19:38:36 -0400] cheeseplus: all the plusses
[2012-08-11 19:39:22 -0400] cheeseplus: hakunin: don't stress too much about the best best practices just yet, seems like you have plenty to worry about so solve the most immediate solution and monitor the hell out of things to make a better future decision
[2012-08-11 19:39:35 -0400] cheeseplus: s/solution/problem/
[2012-08-11 19:40:10 -0400] hakunin: cheeseplus: stormerider: hoover_damm: i'm very appreciating the info you guys give, hate to be so whiny, i'm just really struggling to find decent info and resources. in fact having your advice here is invaluable. i'm willing to pay for advice btw.
[2012-08-11 19:40:37 -0400] stormerider: especially since my prod environment is currently 100ish bare metal boxes, im not exactly thinking along the same kind of lines as you are, i dont want to feed you misinformation. but yeah, IMO go with "what works for right now, with as much monitoring as possible, and scale up and clean up in the future as the design evolves".
[2012-08-11 19:40:46 -0400] cheeseplus: hakunin: welcome to the lovely world of open source solutions!!
[2012-08-11 19:41:23 -0400] cheeseplus: hakunin: same boat as stormerider (we use baremetal for everything)
[2012-08-11 19:41:30 -0400] stormerider: i would suggest in addition to cloudwatch stuff that you also look at something like pingdom.com/mon.itor.us to do external checks of your site, in case AWS itself is having issues.
[2012-08-11 19:41:48 -0400] cheeseplus: also, nothing webfacing so my love of haproxy is very much tied to it's performance in rack
[2012-08-11 19:42:03 -0400] stormerider: new framework is AWS, which is where im using chef, but im still figuring out best design for everything there.
[2012-08-11 19:43:11 -0400] hakunin: yeah that's the plan, we do use pingdom, i wanted to setup something like collectd/graphite
[2012-08-11 19:43:49 -0400] stormerider: hakunin: also, im not sure what your timetable is for hiring a sysadmin. if you're looking to find a consultant, #lopsa might be able to help but i dont know the channel policies about soliciting for work.
[2012-08-11 19:44:11 -0400] cheeseplus: hakunin: not to sway you too much, but give riemann a look in the monitoring space
[2012-08-11 19:44:37 -0400] hakunin: interesting, never heard
[2012-08-11 19:44:47 -0400] hakunin: i'm relying a lot on existing recipes
[2012-08-11 19:44:49 -0400] hakunin: for chef
[2012-08-11 19:45:36 -0400] hakunin: stormerider: yeah, i think i will have to get this stuff setup before hiring at this point, cause if i divert my attention it will take too long
[2012-08-11 19:45:36 -0400] hoover_damm: hakunin, if you can do graphite for monitoring ... it'll take you far
[2012-08-11 19:45:52 -0400] hakunin: hoover_damm: yeah i'm going to try
[2012-08-11 19:45:54 -0400] hoover_damm: hakunin, statsd / or using collectd to send metrics or (diamond) to graphite
[2012-08-11 19:46:02 -0400] stormerider: in my experience, from what ive looked into, cloudwatch is good for metrics in AWS, since you can write custom scripts to push server metrics up into cloudwatch. but its not so good at alerting; it has no concept of acknowledging an alarm. which means for a site like the one i work for, that's bad, very bad, because an outage will kill the pager (assuming i dont first).
[2012-08-11 19:46:07 -0400] hoover_damm: hakunin, then you can tie umpire in front for some graphite monitoring
[2012-08-11 19:46:12 -0400] hoover_damm: or some other graphite alternatives
[2012-08-11 19:46:25 -0400] hoover_damm: http://pinboard.in/u:adam.gibbins/t:graphite
[2012-08-11 19:46:31 -0400] hoover_damm: is a really good bookmark set of graphite items
[2012-08-11 19:46:38 -0400] hoover_damm: (one of the guys in #graphite maintains it)
[2012-08-11 19:46:52 -0400] hakunin: hoover_damm: nice!
[2012-08-11 19:47:30 -0400] stormerider: hakunin: makes sense, pretty reasonable approach if you're under deadline. especially if you can find the time to work with the sysadmin when they come onboard to explain design decisions until they unravel things.
[2012-08-11 19:48:23 -0400] cheeseplus: ^
[2012-08-11 19:48:47 -0400] hakunin: stormerider: yeah, can't wait to be able to delegate this. i'm a dev, and we have so much to do
[2012-08-11 19:49:25 -0400] hakunin: gotta admit however, i love learning about this, if only it wasn't so pressing, with so many experience-based questions
[2012-08-11 19:49:45 -0400] hakunin: unanswered
[2012-08-11 19:54:21 -0400] hakunin: not sure how i'm going to determine number of workers per ec2 instance, and which instances should i use. my current plan draft: use 3 medium instances, 2 unicorn workers each, but one of them also being load-balancing nginx. Not sure if that's bad. Bring postgres into a separate medium. Have a small instance or 2 running background jobs with resque (mostly complex imagemagick processing), and extra small one for CI perhaps.
[2012-08-11 20:03:32 -0400] hoover_damm: http://en.wikipedia.org/wiki/Technical_debt
[2012-08-11 20:03:47 -0400] hoover_damm: if you want a actual good paper on technical debt http://queue.acm.org/detail.cfm?id=2168798
[2012-08-11 20:04:46 -0400] stormerider: hakunin: if you can afford it, toss as much resources at the problem as you can, it will buy you leeway until you can refactor. :)
[2012-08-11 20:05:26 -0400] stormerider: i also tend to prefer the c1 nodes vs the m1 nodes but that's just personal bias; i almost always use more CPU than memory.
[2012-08-11 20:05:54 -0400] hoover_damm: I'll stick to metal
[2012-08-11 20:06:15 -0400] hoover_damm: we do about 200 r/s on a c1medium and on our new metal we just procured we are doing 450r/s
[2012-08-11 20:06:35 -0400] hoover_damm: just some E3-1230 V2's
[2012-08-11 20:06:38 -0400] hoover_damm: :)
[2012-08-11 20:07:15 -0400] stormerider: hoover_damm: that's not really relevant to their plans, at least not right now. if they dont have time to hire an ops person they certainly dont have time to buy hardware and/or provision a datacenter configuration.
[2012-08-11 20:08:33 -0400] hakunin: stormerider: yeah, that's what i'm going for. i'm just trying to get a setup that is relatively trivial to scale by throwing money at it (e.g. add instances easily)
[2012-08-11 20:08:36 -0400] cheeseplus: I'm with hoover_damm on sticking to metal but for hakunin's particular issue I'm stormerider
[2012-08-11 20:08:45 -0400] cheeseplus: I'm with
[2012-08-11 20:08:59 -0400] cheeseplus: I haven't even started drinking today
[2012-08-11 20:09:00 -0400] cheeseplus: I swear
[2012-08-11 20:09:05 -0400] stormerider: AWS is perfect for startups who need to iterate a lot fast. at scale when things are more stable, thats when bare metal shines.
[2012-08-11 20:10:02 -0400] hakunin: i myself was looking at the likes of hetzner: http://www.hetzner.co.za/index.php and was considering maybe one metal machine for serving the site, and ec2 for image processing work
[2012-08-11 20:10:09 -0400] hakunin: but again, time constraints
[2012-08-11 20:10:16 -0400] hakunin: i do realize incredible performance wins on metal
[2012-08-11 20:11:15 -0400] stormerider: hakunin: and if you implement your configs in chef, you should be able to generate nodes on either metal or AWS that have the same config. so you can do testing and spin up and throw away instances on AWS until you hammer out the config and then migrate it to metal.
[2012-08-11 20:11:49 -0400] cheeseplus: hakunin: it always depends on the problem being solved and the relative cost, since my problem space is now mostly riak-centric I'm almost always dealing with bare metal
[2012-08-11 20:11:52 -0400] hakunin: stormerider: exactly why i switched away from my original choice of provisioning tool: rubber. i have everything written in chef atm
[2012-08-11 20:12:12 -0400] stormerider: hakunin: that will make your eventual sysadmin's job much easier :)
[2012-08-11 20:12:35 -0400] hakunin: stormerider: by "everything" i mean i wrote scripts that let me setup stagings on demand, so now can focus on production arrangement without worrying much about provisioning details
[2012-08-11 20:13:14 -0400] hakunin: stormerider: thanks for the most part to hoover_damm's scripts, which i've based mine off of
[2012-08-11 20:13:50 -0400] stormerider: :)
[2012-08-11 20:14:51 -0400] stormerider: crap, forgot to take meds, no wonder why i hurt.
[2012-08-11 20:17:20 -0400] hoover_damm: stormerider, actually we don't either
[2012-08-11 20:17:29 -0400] hoover_damm: stormerider, our hosting provider has a 100% SLA and is half the cost of amazon
[2012-08-11 20:17:49 -0400] hoover_damm: stormerider, people often forget that times evolve
[2012-08-11 20:18:39 -0400] hoover_damm: and truthfully we have more optimization to do, those are just rough numbers
[2012-08-11 20:19:17 -0400] hoover_damm: there is a great illusion that if you want to to metal you have to hire someone competent... this is somewhat true
[2012-08-11 20:19:26 -0400] stormerider: not sure i follow the "we don't either" part
[2012-08-11 20:19:27 -0400] hoover_damm: only to help you negotiate what you don't know
[2012-08-11 20:20:25 -0400] hoover_damm: AWS is a pattern that i'm likely to follow other people on and eventually put on my resume 'No Amazon Web Services clients please'
[2012-08-11 20:20:54 -0400] hoover_damm: I've busted my ass and worked 4x as hard the past 3 years with the cloud
[2012-08-11 20:21:08 -0400] hoover_damm: i'm tired of overworking myself for naught
[2012-08-11 20:22:20 -0400] hoover_damm: and as an operations person who has been doing administration for the most part of the last 18 years (in one way or another) I find it egregious how companies are willing to put their stuff on some random hosting providers datacenter and not care about anything
[2012-08-11 20:22:41 -0400] hoover_damm: I look forward to SWIP'ing ip blocks
[2012-08-11 20:22:45 -0400] hoover_damm: and having native ipv6 before 2015
[2012-08-11 20:23:09 -0400] hoover_damm: and working with old patterns that have worked since 1996
[2012-08-11 20:23:21 -0400] hoover_damm: without creating debt
[2012-08-11 20:23:34 -0400] hoover_damm: It's me exploring and re-evaluating with my client
[2012-08-11 20:24:16 -0400] hoover_damm: although I do love talking to some clients who tell me they have to hire some special 'metal migration' person that they can never find...
[2012-08-11 20:24:22 -0400] stormerider: i think that my point is, just because AWS hasnt been a good experience for your needs doesnt mean it doesnt serve anyone, which is the kind of attitude that you convey.
[2012-08-11 20:25:00 -0400] stormerider: and on that note, im off for a bit, meds and some time for wow now that crunch time is over.
[2012-08-11 20:25:05 -0400] hoover_damm: AWS tries to make you want it
[2012-08-11 20:25:34 -0400] hoover_damm: and sorry to burst your bubble stormerider but the facts don't lie... spend the time and evaluate some current cpu's and evaluate amazon
[2012-08-11 20:25:43 -0400] hoover_damm: put the math together and come up with your own conclusions
[2012-08-11 20:26:02 -0400] hoover_damm: Amazon is a great way for startups to build a presence overnight
[2012-08-11 20:26:06 -0400] hoover_damm: it's also hella hard to move away from
[2012-08-11 20:26:08 -0400] hoover_damm: ask foursquared
[2012-08-11 20:27:33 -0400] hoover_damm: my employer 8months ago was paying almost a million to EC2/month
[2012-08-11 20:27:35 -0400] cheeseplus: hoover_damm: basically everything you've said == my thoughts
[2012-08-11 20:27:40 -0400] hoover_damm: time to understand technical debt
[2012-08-11 20:27:46 -0400] hoover_damm: and realize that the world isnot flat
[2012-08-11 20:28:01 -0400] hakunin: hoover_damm: i don't think it's hard to move away from aws if you're scripting your setup
[2012-08-11 20:28:12 -0400] hoover_damm: cheeseplus, :) I'm just an old coot who keeps trying new things and learning
[2012-08-11 20:28:17 -0400] hakunin: hoover_damm: even in my awkward attempts i see a fairly clear painless path to get out of aws
[2012-08-11 20:28:28 -0400] cheeseplus: hoover_damm: I'm starting to feel like one, maybe just cause I started so young
[2012-08-11 20:28:55 -0400] cheeseplus: never had to do cloud for much work, always doing bare metal
[2012-08-11 20:28:57 -0400] hoover_damm: cheeseplus, it started for me when I realized I needed to work smarter, not harder.
[2012-08-11 20:29:09 -0400] hoover_damm: cheeseplus, it became obsessive as I started reducing my overworkiing habbits
[2012-08-11 20:30:03 -0400] hoover_damm: hakunin, you get used to ebs volumes, load balancers and a lot of api stuff...
[2012-08-11 20:30:05 -0400] cheeseplus: having recently setup a colo and even with all the problems we encountered, I'd take it any day
[2012-08-11 20:30:26 -0400] hoover_damm: hakunin, and suddenly you get thrown okay i want to have x with y... and if you want more you have to hotswap a new disk on or
[2012-08-11 20:30:32 -0400] cheeseplus: does AWS even offer 10Gbit interconnects?
[2012-08-11 20:30:40 -0400] hoover_damm: hakunin, it'smore or less just a different pattern of thought that makes operations have to change their view
[2012-08-11 20:30:49 -0400] hoover_damm: hakunin, if they are capable of doing this :)
[2012-08-11 20:30:54 -0400] hoover_damm: hakunin, i'm watching some try and try
[2012-08-11 20:30:57 -0400] hoover_damm: cheeseplus, yes
[2012-08-11 20:31:19 -0400] hakunin: hoover_damm: you can move away gradually. let's say  get the web/app and db out, keep s3 and cdn, and some heavy background jobs for possible spikes
[2012-08-11 20:31:19 -0400] hoover_damm: cheeseplus, only on the hvm instances (and i think some of the ebs enhanced crap)
[2012-08-11 20:31:31 -0400] cheeseplus: gotcha
[2012-08-11 20:31:36 -0400] hoover_damm: hakunin, let me tell you a secret.  Most people use direct connect toescape amazon
[2012-08-11 20:31:45 -0400] cheeseplus: do they guarantee they are even in the same rack?
[2012-08-11 20:32:04 -0400] hoover_damm: hakunin, hosting with Equinix is FREAKING expensive
[2012-08-11 20:32:27 -0400] hoover_damm: hakunin, I can get 2 racks in New York for the same price of 1 rack in Ashburn
[2012-08-11 20:32:35 -0400] hoover_damm: and they know it
[2012-08-11 20:32:53 -0400] hoover_damm: hakunin, they're using ec2 as leverage to make more money :) building out new buildings so they can charge more
[2012-08-11 20:33:36 -0400] cheeseplus: we are using the markley group out in Boston
[2012-08-11 20:33:38 -0400] cheeseplus: <3
[2012-08-11 20:34:22 -0400] hoover_damm: you can get a private line down to ashburn
[2012-08-11 20:34:25 -0400] hoover_damm: and then do it
[2012-08-11 20:34:28 -0400] hoover_damm: but meh
[2012-08-11 20:35:04 -0400] hoover_damm: basically my stuff in ec2 is about 8ms from New York
[2012-08-11 20:35:09 -0400] hoover_damm: a little over inter-zone latency
[2012-08-11 20:35:34 -0400] hoover_damm: my client did ebs raiding
[2012-08-11 20:35:37 -0400] hoover_damm: we're now using ssd
[2012-08-11 20:35:50 -0400] hoover_damm: we have come clean (to ourselves) about how incredibly stupid EBS raiding is
[2012-08-11 20:36:23 -0400] cheeseplus: hoover_damm: that makes my brain hurt
[2012-08-11 20:37:08 -0400] hoover_damm: what does?
[2012-08-11 20:38:06 -0400] cheeseplus: the EBS raiding
[2012-08-11 20:38:51 -0400] hoover_damm: oh well basically it's simple amazon uses tcp/ip to provide the ebs block devices (via gndb)
[2012-08-11 20:38:58 -0400] hoover_damm: there's only 1 uplink per instance
[2012-08-11 20:39:20 -0400] hoover_damm: so if you have a 10mb uplink and you slap 60 disks together and make them use the same tcp/ip path to get out
[2012-08-11 20:39:27 -0400] hoover_damm: sure you'll gain something
[2012-08-11 20:39:33 -0400] cheeseplus: I was more agreeing with the sentiment of how stupid it is ;)
[2012-08-11 20:39:36 -0400] hoover_damm: but the price is high
[2012-08-11 20:39:44 -0400] hoover_damm: ahh :) yeah it does make the brain hurt
[2012-08-11 20:39:51 -0400] hoover_damm: it's like once you put some of the pieces together it's like wuh?
[2012-08-11 20:40:00 -0400] cheeseplus: (insert the jackie chan meme face here)
[2012-08-11 20:40:03 -0400] hoover_damm: how can anyone be this silly (other then ignorance)
[2012-08-11 20:40:16 -0400] hoover_damm: obviously the answer is ignorance and marketing
[2012-08-11 20:40:21 -0400] cheeseplus: http://i0.kym-cdn.com/entries/icons/original/000/004/592/my-brain-is-full-of-fuck.jpg
[2012-08-11 20:40:34 -0400] hoover_damm: yep
[2012-08-11 20:40:40 -0400] cheeseplus: the url for that is highly appropirate for how I feel about that
[2012-08-11 20:41:14 -0400] hoover_damm: as i'm doing almost 3500 write lately
[2012-08-11 20:41:16 -0400] hoover_damm: i/ops
[2012-08-11 20:41:26 -0400] hoover_damm: there's really no way i could approach tcpip
[2012-08-11 20:41:40 -0400] rayrod2030: :)
[2012-08-11 20:41:53 -0400] hoover_damm: i'm sure amazon will offer fiber based uplinks
[2012-08-11 20:41:57 -0400] hoover_damm: and fiber disks in a year or two
[2012-08-11 20:42:18 -0400] hoover_damm: they're making enough money off of you
[2012-08-11 20:42:19 -0400] hoover_damm: to do it
[2012-08-11 20:42:30 -0400] cheeseplus: hoover_damm: we've been playing with 10Gbit inside a rack and topped it out with riak
[2012-08-11 20:42:49 -0400] hoover_damm: cheeseplus, the backplane?
[2012-08-11 20:42:50 -0400] cheeseplus: that 20% overhead for TCP/IP is no joke
[2012-08-11 20:42:53 -0400] cheeseplus: yep
[2012-08-11 20:42:58 -0400] cheeseplus: well TCP/IP
[2012-08-11 20:43:02 -0400] hoover_damm: cheeseplus, needed a bigger backplane
[2012-08-11 20:43:03 -0400] hoover_damm: and yeah
[2012-08-11 20:43:09 -0400] hoover_damm: what kinda switches did you have on that?
[2012-08-11 20:43:11 -0400] hoover_damm: out of curiosity
[2012-08-11 20:43:15 -0400] cheeseplus: Force10
[2012-08-11 20:43:30 -0400] cheeseplus: Dell just bought them so we may be looking at others
[2012-08-11 20:43:45 -0400] hoover_damm: I haven't decided where I want to go other then Cisco *cry*
[2012-08-11 20:43:50 -0400] cheeseplus: it's pretty awesome to watch each node pushing 7-8GB
[2012-08-11 20:44:01 -0400] cheeseplus: hoover_damm: we hear very good things about arista
[2012-08-11 20:44:11 -0400] hoover_damm: how well does Riak handle numa?
[2012-08-11 20:44:14 -0400] cheeseplus: and probably what we will go with next
[2012-08-11 20:44:25 -0400] hoover_damm: we're very numa oriented lately as we are getting new hardware
[2012-08-11 20:44:36 -0400] hoover_damm: our postgresql has loved it
[2012-08-11 20:44:40 -0400] hoover_damm: but our mongodb is suffering badly
[2012-08-11 20:44:41 -0400] cheeseplus: hoover_damm: not sure if we've actually investigated that
[2012-08-11 20:45:28 -0400] hoover_damm: monday i'm going to finish out my preseed image so my installed servers come out perfectly
[2012-08-11 20:45:39 -0400] cheeseplus: for a second my meme addled brain jumped to numa numa: http://www.youtube.com/watch?v=60og9gwKh1o
[2012-08-11 20:45:43 -0400] hoover_damm: and then after that i'm going to dig into the AES-NI optimization
[2012-08-11 20:45:52 -0400] hoover_damm: we don't do a lot of AES ourselves
[2012-08-11 20:45:55 -0400] cheeseplus: damn, taking it down the stack
[2012-08-11 20:45:56 -0400] hoover_damm: but we will in time
[2012-08-11 20:46:32 -0400] cheeseplus: we have very few customers that I am aware of that can take advantage of all their hardware
[2012-08-11 20:46:37 -0400] cheeseplus: wrt to riak
[2012-08-11 20:46:56 -0400] cheeseplus: hence why we got the new colo, trying to see where we top out
[2012-08-11 20:47:04 -0400] hoover_damm: i bet
[2012-08-11 20:47:13 -0400] hoover_damm: basho's prolly loving it up helping you scale
[2012-08-11 20:47:18 -0400] cheeseplus: so far it seems that network is going to be the bottleneck long before cpu, ram, and riak
[2012-08-11 20:47:23 -0400] cheeseplus: hoover_damm: oh I work for them ;)
[2012-08-11 20:47:35 -0400] hoover_damm: ahh that explains it
[2012-08-11 20:47:44 -0400] hoover_damm: but I know people there
[2012-08-11 20:47:50 -0400] hoover_damm: say hi to Grant and Justin and John for me
[2012-08-11 20:48:04 -0400] cheeseplus: I'll be seeing them at the conference in October
[2012-08-11 20:48:08 -0400] hoover_damm: typical of ex-engineyard lol
[2012-08-11 20:48:15 -0400] hoover_damm: knowing only the ex-ey folks
[2012-08-11 20:48:16 -0400] cheeseplus: hehe, yep
[2012-08-11 20:48:26 -0400] hoover_damm: basho west?
[2012-08-11 20:48:28 -0400] cheeseplus: which Justin?
[2012-08-11 20:48:32 -0400] cheeseplus: Shoffstall?
[2012-08-11 20:48:57 -0400] cheeseplus: hoover_damm: yea, we are throwing our own conference and bringing everyone out to Basho West
[2012-08-11 20:49:07 -0400] hoover_damm: cheeseplus, peace
[2012-08-11 20:49:15 -0400] hoover_damm: jp
[2012-08-11 20:49:31 -0400] cheeseplus: ah, feels like we have too many justin's
[2012-08-11 20:49:33 -0400] cheeseplus: and seans
[2012-08-11 20:49:35 -0400] cheeseplus: http://basho.com/community/ricon2012/
[2012-08-11 20:49:53 -0400] hoover_damm: cheeseplus, when I joined EY basically my employer was on drugs and I was looking for an escape... so I joined the support team and then moved up in EY
[2012-08-11 20:50:17 -0400] hoover_damm: i've mentioned the story a few times in irc ...
[2012-08-11 20:50:49 -0400] hoover_damm: allowed me to focus on my web operations skills and stack
[2012-08-11 20:50:52 -0400] hoover_damm: which is good
[2012-08-11 20:51:07 -0400] cheeseplus: nice
[2012-08-11 20:51:27 -0400] hoover_damm: i don't want to ever be useless
[2012-08-11 20:51:27 -0400] cheeseplus: I made the exodus from the game industry
[2012-08-11 20:51:53 -0400] cheeseplus: I also have the / approve of the well rounded background
[2012-08-11 20:52:16 -0400] cheeseplus: gotta keep myself on my toes
[2012-08-11 20:52:31 -0400] hoover_damm: whatever allows us to use our long term memory and create procedural memories
[2012-08-11 20:52:37 -0400] hoover_damm: is awesome tbh
[2012-08-11 20:53:26 -0400] cheeseplus: things have stopped looking so unique for me
[2012-08-11 20:53:35 -0400] cheeseplus: in a positive way
[2012-08-11 20:53:36 -0400] hoover_damm: that's usually when you need change
[2012-08-11 20:53:51 -0400] hoover_damm: and if you can see it early enough and respond and provide that change
[2012-08-11 20:53:55 -0400] hoover_damm: you can leave that ugly feeling fast
[2012-08-11 20:54:01 -0400] cheeseplus: I guess I mean with regard to many of the problems
[2012-08-11 20:54:09 -0400] hoover_damm: same thing :)
[2012-08-11 20:54:16 -0400] cheeseplus: everyone is like "we have this totally special problem", most of the time they really don't
[2012-08-11 20:54:31 -0400] hoover_damm: that's because everyone thinks they are special and unique
[2012-08-11 20:54:49 -0400] cheeseplus: with riak at least, the problems are still things like network/cpu/ram, but the scale is what is fun
[2012-08-11 20:55:07 -0400] cheeseplus: that and we can melt a switch if we try hard
[2012-08-11 20:55:36 -0400] hoover_damm: anything can melt a switch with enough effort
[2012-08-11 20:55:52 -0400] hoover_damm: I understand your pride of it because it is something to be proud of
[2012-08-11 20:55:58 -0400] cheeseplus: this is true
[2012-08-11 20:56:17 -0400] hoover_damm: Basically look inwards on the problem sometime
[2012-08-11 20:56:18 -0400] cheeseplus: we almost accidentally melted one not too long ago, turns out dell sent switches with fans that blew the wrong way
[2012-08-11 20:56:20 -0400] hoover_damm: and turn it upside down
[2012-08-11 20:56:21 -0400] hoover_damm: and have fun
[2012-08-11 20:56:23 -0400] hoover_damm: always have fun
[2012-08-11 20:56:31 -0400] hoover_damm: and have a good day
[2012-08-11 20:56:35 -0400] cheeseplus: later
[2012-08-11 21:11:45 -0400] ssd7: Ohai Chefs!
